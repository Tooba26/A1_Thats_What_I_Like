{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec or GloVe embeddings\n",
    "def load_embeddings(embedding_path, binary=True):\n",
    "    \"\"\"Load pre-trained embeddings.\"\"\"\n",
    "    return KeyedVectors.load_word2vec_format(embedding_path, binary=binary)\n",
    "\n",
    "# Parse the analogy dataset to extract specific sections\n",
    "def parse_analogy_dataset(file_path, sections):\n",
    "    \"\"\"Extract specific sections (e.g., 'capital-common-countries', 'past-tense') from the analogy dataset.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    extracted_data = []\n",
    "    capture = False\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        if line.startswith(':'):\n",
    "            capture = any(section in line for section in sections)\n",
    "        elif capture:\n",
    "            extracted_data.append(line.split())\n",
    "    return extracted_data\n",
    "\n",
    "# Perform analogy evaluation\n",
    "def evaluate_analogies(embeddings, analogy_data):\n",
    "    \"\"\"Evaluate accuracy of word embeddings on analogy data.\"\"\"\n",
    "    correct = 0\n",
    "    total = len(analogy_data)\n",
    "\n",
    "    for analogy in analogy_data:\n",
    "        a, b, c, expected = analogy\n",
    "        if all(word in embeddings for word in [a, b, c, expected]):\n",
    "            # Compute vector operation\n",
    "            result_vector = embeddings[b] - embeddings[a] + embeddings[c]\n",
    "            # Find the most similar word\n",
    "            predicted, _ = embeddings.most_similar(positive=[result_vector], topn=1)[0]\n",
    "            if predicted == expected:\n",
    "                correct += 1\n",
    "    return correct, total\n",
    "\n",
    "# Main function to run evaluation\n",
    "def main():\n",
    "    # Paths to files\n",
    "    analogy_file_path = \"word-test.v1.txt\"  # Replace with the actual file path\n",
    "    embedding_path = \"path_to_pretrained_embeddings\"  # Replace with your embeddings file\n",
    "\n",
    "    # Sections for evaluation\n",
    "    semantic_sections = [\"capital-common-countries\"]\n",
    "    syntactic_sections = [\"past-tense\"]\n",
    "\n",
    "    # Load embeddings\n",
    "    embeddings = load_embeddings(embedding_path, binary=True)  # Set binary=False for GloVe\n",
    "\n",
    "    # Extract data\n",
    "    semantic_data = parse_analogy_dataset(analogy_file_path, semantic_sections)\n",
    "    syntactic_data = parse_analogy_dataset(analogy_file_path, syntactic_sections)\n",
    "\n",
    "    # Evaluate on both tasks\n",
    "    semantic_correct, semantic_total = evaluate_analogies(embeddings, semantic_data)\n",
    "    syntactic_correct, syntactic_total = evaluate_analogies(embeddings, syntactic_data)\n",
    "\n",
    "    # Calculate and print accuracies\n",
    "    semantic_accuracy = (semantic_correct / semantic_total) * 100\n",
    "    syntactic_accuracy = (syntactic_correct / syntactic_total) * 100\n",
    "\n",
    "    print(f\"Semantic Accuracy ({semantic_sections}): {semantic_accuracy:.2f}%\")\n",
    "    print(f\"Syntactic Accuracy ({syntactic_sections}): {syntactic_accuracy:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_accuracy(questions, word_vectors, word2index):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for analogy questions.\n",
    "    \n",
    "    Args:\n",
    "        questions (list): List of analogy questions in format 'a b c d'.\n",
    "        word_vectors (np.array): Pre-trained word vectors.\n",
    "        word2index (dict): Mapping of words to their indices in `word_vectors`.\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for question in questions:\n",
    "        try:\n",
    "            a, b, c, d = question.split()\n",
    "            if a in word2index and b in word2index and c in word2index and d in word2index:\n",
    "                vec_a = word_vectors[word2index[a]]\n",
    "                vec_b = word_vectors[word2index[b]]\n",
    "                vec_c = word_vectors[word2index[c]]\n",
    "                \n",
    "                # Compute vector: b - a + c\n",
    "                target_vector = vec_b - vec_a + vec_c\n",
    "                # Find most similar word\n",
    "                similarities = cosine_similarity(target_vector.reshape(1, -1), word_vectors)\n",
    "                predicted_index = np.argmax(similarities)\n",
    "                predicted_word = list(word2index.keys())[predicted_index]\n",
    "\n",
    "                if predicted_word == d:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        except Exception as e:\n",
    "            continue  # Skip malformed questions or missing words\n",
    "\n",
    "    return (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "# Load the dataset and split into semantic and syntactic subsets\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    semantic_questions = []\n",
    "    syntactic_questions = []\n",
    "    current_section = None  # Initialize current_section\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\": capital-common-countries\"):\n",
    "            current_section = \"semantic\"\n",
    "        elif line.startswith(\": past-tense\"):\n",
    "            current_section = \"syntactic\"\n",
    "        elif line.startswith(\":\"):\n",
    "            current_section = None  # Reset for unrelated sections\n",
    "        elif current_section == \"semantic\":\n",
    "            semantic_questions.append(line.strip())\n",
    "        elif current_section == \"syntactic\":\n",
    "            syntactic_questions.append(line.strip())\n",
    "    \n",
    "    return semantic_questions, syntactic_questions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy: 0.00%\n",
      "Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word_vectors = np.random.rand(1000, 300)  # Replace with actual embeddings\n",
    "word2index = {f\"word{i}\": i for i in range(1000)}  # Replace with actual vocabulary\n",
    "\n",
    "semantic_questions, syntactic_questions = load_dataset(\"word-test.v1.txt\")\n",
    "\n",
    "semantic_accuracy = calculate_accuracy(semantic_questions, word_vectors, word2index)\n",
    "syntactic_accuracy = calculate_accuracy(syntactic_questions, word_vectors, word2index)\n",
    "\n",
    "print(f\"Semantic Accuracy: {semantic_accuracy:.2f}%\")\n",
    "print(f\"Syntactic Accuracy: {syntactic_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
